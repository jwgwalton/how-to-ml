{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "\n",
    "This is an example notebook for using the expressiveness of notebooks for EDA of a dataset before moving onto the robustness of pure python scripts for modelling and serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "print(iris[\"DESCR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a dataset for classifying species of iris' based of measurements of their petals and sepals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['setosa', 'versicolor', 'virginica']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(iris.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use this database as a simple example project to show the end to end ML workflow and how to structure it for \n",
    "# use in a production system and for working on in a team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
    "                     columns= iris['feature_names'] + ['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "3                4.6               3.1                1.5               0.2   \n",
       "4                5.0               3.6                1.4               0.2   \n",
       "\n",
       "   target  \n",
       "0     0.0  \n",
       "1     0.0  \n",
       "2     0.0  \n",
       "3     0.0  \n",
       "4     0.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "3                4.6               3.1                1.5               0.2   \n",
       "4                5.0               3.6                1.4               0.2   \n",
       "\n",
       "   target species  \n",
       "0     0.0  setosa  \n",
       "1     0.0  setosa  \n",
       "2     0.0  setosa  \n",
       "3     0.0  setosa  \n",
       "4     0.0  setosa  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have the species encoded as a number which is useful for modelling but it's going to see the actual species\n",
    "species = {\n",
    "    0: \"setosa\",\n",
    "    1: \"versicolor\",\n",
    "    2: \"virginica\"\n",
    "}\n",
    "df[\"species\"] = df[\"target\"].apply(lambda x: species[x])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 6 columns):\n",
      "sepal length (cm)    150 non-null float64\n",
      "sepal width (cm)     150 non-null float64\n",
      "petal length (cm)    150 non-null float64\n",
      "petal width (cm)     150 non-null float64\n",
      "target               150 non-null float64\n",
      "species              150 non-null object\n",
      "dtypes: float64(5), object(1)\n",
      "memory usage: 7.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No nulls to impute, (real life is never this easy)\n",
    "# what are some summary statistics of the variables to get an understading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.843333</td>\n",
       "      <td>3.057333</td>\n",
       "      <td>3.758000</td>\n",
       "      <td>1.199333</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.828066</td>\n",
       "      <td>0.435866</td>\n",
       "      <td>1.765298</td>\n",
       "      <td>0.762238</td>\n",
       "      <td>0.819232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.300000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.100000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.400000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sepal length (cm)  sepal width (cm)  petal length (cm)  \\\n",
       "count         150.000000        150.000000         150.000000   \n",
       "mean            5.843333          3.057333           3.758000   \n",
       "std             0.828066          0.435866           1.765298   \n",
       "min             4.300000          2.000000           1.000000   \n",
       "25%             5.100000          2.800000           1.600000   \n",
       "50%             5.800000          3.000000           4.350000   \n",
       "75%             6.400000          3.300000           5.100000   \n",
       "max             7.900000          4.400000           6.900000   \n",
       "\n",
       "       petal width (cm)      target  \n",
       "count        150.000000  150.000000  \n",
       "mean           1.199333    1.000000  \n",
       "std            0.762238    0.819232  \n",
       "min            0.100000    0.000000  \n",
       "25%            0.300000    0.000000  \n",
       "50%            1.300000    1.000000  \n",
       "75%            1.800000    2.000000  \n",
       "max            2.500000    2.000000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It always makes sense to scale these quantitative variables \n",
    "# but as we can see here they are defintely on different length scales \n",
    "# so scaling will be important to understand feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0    50\n",
       "1.0    50\n",
       "0.0    50\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Are the target variables equally distributed? If they are not we can get a biased classifier when optimising for accuracy\n",
    "# as it will preferentially classify to the majority class\n",
    "\n",
    "df[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.117570</td>\n",
       "      <td>0.871754</td>\n",
       "      <td>0.817941</td>\n",
       "      <td>0.782561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <td>-0.117570</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.428440</td>\n",
       "      <td>-0.366126</td>\n",
       "      <td>-0.426658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>petal length (cm)</th>\n",
       "      <td>0.871754</td>\n",
       "      <td>-0.428440</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.962865</td>\n",
       "      <td>0.949035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>petal width (cm)</th>\n",
       "      <td>0.817941</td>\n",
       "      <td>-0.366126</td>\n",
       "      <td>0.962865</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.956547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <td>0.782561</td>\n",
       "      <td>-0.426658</td>\n",
       "      <td>0.949035</td>\n",
       "      <td>0.956547</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   sepal length (cm)  sepal width (cm)  petal length (cm)  \\\n",
       "sepal length (cm)           1.000000         -0.117570           0.871754   \n",
       "sepal width (cm)           -0.117570          1.000000          -0.428440   \n",
       "petal length (cm)           0.871754         -0.428440           1.000000   \n",
       "petal width (cm)            0.817941         -0.366126           0.962865   \n",
       "target                      0.782561         -0.426658           0.949035   \n",
       "\n",
       "                   petal width (cm)    target  \n",
       "sepal length (cm)          0.817941  0.782561  \n",
       "sepal width (cm)          -0.366126 -0.426658  \n",
       "petal length (cm)          0.962865  0.949035  \n",
       "petal width (cm)           1.000000  0.956547  \n",
       "target                     0.956547  1.000000  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As the target is equally distributed we will not have this problem\n",
    "\n",
    "# What about correlation between any of the variables?\n",
    "# Are there redundant variables which can be removed?\n",
    "# A simpler model is always preferrable but certain models assume independent variables\n",
    "# so you should not be using these models with correlated variables.\n",
    "\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Petal length and width are highly correlated \n",
    "# (as would be expected if you think of the analogy of height and weight in a person)\n",
    "\n",
    "# What is very interesting though is how correlated petal width and length are with the target \n",
    "# these are going to be powerful features in predicting the species of iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jwgwalton/anaconda3/lib/python3.7/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "/home/jwgwalton/anaconda3/lib/python3.7/site-packages/statsmodels/nonparametric/kde.py:488: RuntimeWarning: invalid value encountered in true_divide\n",
      "  binned = fast_linbin(X, a, b, gridsize) / (delta * nobs)\n",
      "/home/jwgwalton/anaconda3/lib/python3.7/site-packages/statsmodels/nonparametric/kdetools.py:34: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  FAC1 = 2*(np.pi*bw/RANGE)**2\n",
      "/home/jwgwalton/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:83: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.PairGrid at 0x7f9f07d72358>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We know the high level correlation but a more visual representation of this is always useful.\n",
    "# A pair plot will show us a scatter graph of each feature rather than just a single value for the correlation.\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.pairplot(df, hue='species')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pair plot lets us see the separability of individual features for the different species.\n",
    "\n",
    "# Interestingly it looks like setosa is perfectly separable on petal length or petal width\n",
    "\n",
    "# we could have a heuristic tsuch as \n",
    "# if petal_width < 1.0cm:\n",
    "#    species = \"setosa\"\n",
    "\n",
    "# For versicolor and virginica we will need the complexity of an ML model  to differentiate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "\n",
    "Though i've stated that modelling should be done outside of a notebook (see README.md at top level for discussion)\n",
    "\n",
    "In a notebook we can do model selection and write the final modelling code in a separate script. This allows for better reproducability when someone else is trying to work on your code.\n",
    "\n",
    "\n",
    "### What we've learnt from the EDA\n",
    "- There are no missing values to deal with through either imputation or ignoring the rows\n",
    "- We need to scale the features so they are treated equally in the model (some models are robust to this such as ensembles of trees but it will never affect us badly)\n",
    "- There is no class imbalance to deal with (the species are equally distributed)\n",
    "- Some features are correlated (which may affect models which assume feature independence such as Naive Bayes)\n",
    "\n",
    "\n",
    "### Aims for model selection\n",
    "\n",
    "- Dumb baseline\n",
    "- Compare different algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"target\"].values\n",
    "\n",
    "columns = [\n",
    "    'sepal length (cm)', \n",
    "    'sepal width (cm)', \n",
    "    'petal length (cm)',\n",
    "    'petal width (cm)', \n",
    "]\n",
    "X = df[columns].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always try and use a pipeline object for composability\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Scale data based of the minimum and maximum observed value to the interval [0, 1]\n",
    "scaler = MinMaxScaler()\n",
    "# A good baseline model, can be done for multi class classification and doesn't require independent features so we can throw all the features at it\n",
    "lr = LogisticRegression(multi_class='auto') # specific algorithm for multi class problems\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"scaler\", scaler),\n",
    "    (\"lr\", lr),\n",
    "])\n",
    "\n",
    "fitted_pipeline = pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = fitted_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict   0.0       1.0       2.0       \n",
      "Actual\n",
      "0.0       7         0         0         \n",
      "\n",
      "1.0       0         11        1         \n",
      "\n",
      "2.0       0         0         11        \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Overall Statistics : \n",
      "\n",
      "95% CI                                                            (0.90243,1.0309)\n",
      "ACC Macro                                                         0.97778\n",
      "ARI                                                               0.88499\n",
      "AUNP                                                              0.97368\n",
      "AUNU                                                              0.97734\n",
      "Bennett S                                                         0.95\n",
      "CBA                                                               0.94444\n",
      "CSI                                                               0.94444\n",
      "Chi-Squared                                                       55.20833\n",
      "Chi-Squared DF                                                    4\n",
      "Conditional Entropy                                               0.16553\n",
      "Cramer V                                                          0.95924\n",
      "Cross Entropy                                                     1.55358\n",
      "F1 Macro                                                          0.97101\n",
      "F1 Micro                                                          0.96667\n",
      "FNR Macro                                                         0.02778\n",
      "FNR Micro                                                         0.03333\n",
      "FPR Macro                                                         0.01754\n",
      "FPR Micro                                                         0.01667\n",
      "Gwet AC1                                                          0.95056\n",
      "Hamming Loss                                                      0.03333\n",
      "Joint Entropy                                                     1.71492\n",
      "KL Divergence                                                     0.00418\n",
      "Kappa                                                             0.94889\n",
      "Kappa 95% CI                                                      (0.85041,1.04738)\n",
      "Kappa No Prevalence                                               0.93333\n",
      "Kappa Standard Error                                              0.05025\n",
      "Kappa Unbiased                                                    0.94885\n",
      "Lambda A                                                          0.94444\n",
      "Lambda B                                                          0.94444\n",
      "Mutual Information                                                1.38387\n",
      "NIR                                                               0.4\n",
      "Overall ACC                                                       0.96667\n",
      "Overall CEN                                                       0.07539\n",
      "Overall J                                                         (2.83333,0.94444)\n",
      "Overall MCC                                                       0.95051\n",
      "Overall MCEN                                                      0.11564\n",
      "Overall RACC                                                      0.34778\n",
      "Overall RACCU                                                     0.34833\n",
      "P-Value                                                           0.0\n",
      "PPV Macro                                                         0.97222\n",
      "PPV Micro                                                         0.96667\n",
      "Pearson C                                                         0.80494\n",
      "Phi-Squared                                                       1.84028\n",
      "RCI                                                               0.89317\n",
      "RR                                                                10.0\n",
      "Reference Entropy                                                 1.5494\n",
      "Response Entropy                                                  1.5494\n",
      "SOA1(Landis & Koch)                                               Almost Perfect\n",
      "SOA2(Fleiss)                                                      Excellent\n",
      "SOA3(Altman)                                                      Very Good\n",
      "SOA4(Cicchetti)                                                   Excellent\n",
      "SOA5(Cramer)                                                      Very Strong\n",
      "SOA6(Matthews)                                                    Very Strong\n",
      "Scott PI                                                          0.94885\n",
      "Standard Error                                                    0.03277\n",
      "TNR Macro                                                         0.98246\n",
      "TNR Micro                                                         0.98333\n",
      "TPR Macro                                                         0.97222\n",
      "TPR Micro                                                         0.96667\n",
      "Zero-one Loss                                                     1\n",
      "\n",
      "Class Statistics :\n",
      "\n",
      "Classes                                                           0.0           1.0           2.0           \n",
      "ACC(Accuracy)                                                     1.0           0.96667       0.96667       \n",
      "AGF(Adjusted F-score)                                             1.0           0.94474       0.98557       \n",
      "AGM(Adjusted geometric mean)                                      1.0           0.97339       0.96326       \n",
      "AM(Difference between automatic and manual classification)        0             -1            1             \n",
      "AUC(Area under the ROC curve)                                     1.0           0.95833       0.97368       \n",
      "AUCI(AUC value interpretation)                                    Excellent     Excellent     Excellent     \n",
      "AUPR(Area under the PR curve)                                     1.0           0.95833       0.95833       \n",
      "BCD(Bray-Curtis dissimilarity)                                    0.0           0.01667       0.01667       \n",
      "BM(Informedness or bookmaker informedness)                        1.0           0.91667       0.94737       \n",
      "CEN(Confusion entropy)                                            0             0.09834       0.09834       \n",
      "DOR(Diagnostic odds ratio)                                        None          None          None          \n",
      "DP(Discriminant power)                                            None          None          None          \n",
      "DPI(Discriminant power interpretation)                            None          None          None          \n",
      "ERR(Error rate)                                                   0.0           0.03333       0.03333       \n",
      "F0.5(F0.5 score)                                                  1.0           0.98214       0.9322        \n",
      "F1(F1 score - harmonic mean of precision and sensitivity)         1.0           0.95652       0.95652       \n",
      "F2(F2 score)                                                      1.0           0.9322        0.98214       \n",
      "FDR(False discovery rate)                                         0.0           0.0           0.08333       \n",
      "FN(False negative/miss/type 2 error)                              0             1             0             \n",
      "FNR(Miss rate or false negative rate)                             0.0           0.08333       0.0           \n",
      "FOR(False omission rate)                                          0.0           0.05263       0.0           \n",
      "FP(False positive/type 1 error/false alarm)                       0             0             1             \n",
      "FPR(Fall-out or false positive rate)                              0.0           0.0           0.05263       \n",
      "G(G-measure geometric mean of precision and sensitivity)          1.0           0.95743       0.95743       \n",
      "GI(Gini index)                                                    1.0           0.91667       0.94737       \n",
      "GM(G-mean geometric mean of specificity and sensitivity)          1.0           0.95743       0.97333       \n",
      "IBA(Index of balanced accuracy)                                   1.0           0.84028       0.99723       \n",
      "ICSI(Individual classification success index)                     1.0           0.91667       0.91667       \n",
      "IS(Information score)                                             2.09954       1.32193       1.32193       \n",
      "J(Jaccard index)                                                  1.0           0.91667       0.91667       \n",
      "LS(Lift score)                                                    4.28571       2.5           2.5           \n",
      "MCC(Matthews correlation coefficient)                             1.0           0.93189       0.93189       \n",
      "MCCI(Matthews correlation coefficient interpretation)             Very Strong   Very Strong   Very Strong   \n",
      "MCEN(Modified confusion entropy)                                  0             0.14937       0.14937       \n",
      "MK(Markedness)                                                    1.0           0.94737       0.91667       \n",
      "N(Condition negative)                                             23            18            19            \n",
      "NLR(Negative likelihood ratio)                                    0.0           0.08333       0.0           \n",
      "NLRI(Negative likelihood ratio interpretation)                    Good          Good          Good          \n",
      "NPV(Negative predictive value)                                    1.0           0.94737       1.0           \n",
      "OC(Overlap coefficient)                                           1.0           1.0           1.0           \n",
      "OOC(Otsuka-Ochiai coefficient)                                    1.0           0.95743       0.95743       \n",
      "OP(Optimized precision)                                           1.0           0.92319       0.93964       \n",
      "P(Condition positive or support)                                  7             12            11            \n",
      "PLR(Positive likelihood ratio)                                    None          None          19.0          \n",
      "PLRI(Positive likelihood ratio interpretation)                    None          None          Good          \n",
      "POP(Population)                                                   30            30            30            \n",
      "PPV(Precision or positive predictive value)                       1.0           1.0           0.91667       \n",
      "PRE(Prevalence)                                                   0.23333       0.4           0.36667       \n",
      "Q(Yule Q - coefficient of colligation)                            None          None          None          \n",
      "QI(Yule Q interpretation)                                         None          None          None          \n",
      "RACC(Random accuracy)                                             0.05444       0.14667       0.14667       \n",
      "RACCU(Random accuracy unbiased)                                   0.05444       0.14694       0.14694       \n",
      "TN(True negative/correct rejection)                               23            18            18            \n",
      "TNR(Specificity or true negative rate)                            1.0           1.0           0.94737       \n",
      "TON(Test outcome negative)                                        23            19            18            \n",
      "TOP(Test outcome positive)                                        7             11            12            \n",
      "TP(True positive/hit)                                             7             11            11            \n",
      "TPR(Sensitivity, recall, hit rate, or true positive rate)         1.0           0.91667       1.0           \n",
      "Y(Youden index)                                                   1.0           0.91667       0.94737       \n",
      "dInd(Distance index)                                              0.0           0.08333       0.05263       \n",
      "sInd(Similarity index)                                            1.0           0.94107       0.96278       \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A great package for getting a confusion matrix and other statistics\n",
    "from pycm import ConfusionMatrix\n",
    "\n",
    "confusion_matrix = ConfusionMatrix(actual_vector=y_test, predict_vector=predictions)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsurprisingly the setosa is predicted perfectly as we previously noted that it's perfectly separable. \n",
    "# This is incredibly good performance as we've only got 1 misclassification but lets try some different algorithms to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "lr = LogisticRegression(multi_class='auto')\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"scaler\", scaler),\n",
    "    (\"lr\", lr),\n",
    "])\n",
    "\n",
    "lr_pipeline = pipeline.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "nb = MultinomialNB() # give it a go even though the correlated features may be a problem\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"scaler\", scaler),\n",
    "    (\"nb\", nb),\n",
    "])\n",
    "\n",
    "nb_pipeline = pipeline.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "# This may overfit as it can model complex feature interractions\n",
    "# It has lots of hyperparameters which can vary the model complexity hugely\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"scaler\", scaler),\n",
    "    (\"rf\", rf),\n",
    "])\n",
    "rf_pipeline = pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model assesment\n",
    "\n",
    "Normally i would overlay plots of ROC curves for binary classification problems for easy comparison and we can visually see the the difference, it is possible to do this for multi class but i'm going to compare the confusion matrices and make a choice of the model to use.\n",
    "\n",
    "Note: It is very important to do lots of different assesments of the model validity and dig into the misclassifications to identify commonalities which will often lead to specific feature engineering strategies but we're concnetrating on the end to end process of delivering an ML project rather than tuning hyperparams and the other small gains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "lr_predictions = lr_pipeline.predict(X_test)\n",
    "nb_predictions = nb_pipeline.predict(X_test)\n",
    "rf_predictions = rf_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "\n",
      "Predict   0.0       1.0       2.0       \n",
      "Actual\n",
      "0.0       7         0         0         \n",
      "\n",
      "1.0       0         11        1         \n",
      "\n",
      "2.0       0         0         11        \n",
      "\n",
      "\n",
      "Naive Bayes AUC:\n",
      "\n",
      "Predict   0.0       1.0       2.0       \n",
      "Actual\n",
      "0.0       7         0         0         \n",
      "\n",
      "1.0       0         0         12        \n",
      "\n",
      "2.0       0         1         10        \n",
      "\n",
      "\n",
      "Random Forest AUC:\n",
      "\n",
      "Predict   0.0       1.0       2.0       \n",
      "Actual\n",
      "0.0       7         0         0         \n",
      "\n",
      "1.0       0         11        1         \n",
      "\n",
      "2.0       0         0         11        \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_cm = ConfusionMatrix(actual_vector=y_test, predict_vector=lr_predictions)\n",
    "nb_cm = ConfusionMatrix(actual_vector=y_test, predict_vector=nb_predictions)\n",
    "rf_cm = ConfusionMatrix(actual_vector=y_test, predict_vector=rf_predictions)\n",
    "print(\"Logistic Regression:\\n\")\n",
    "lr_cm.print_matrix()\n",
    "print(\"Naive Bayes AUC:\\n\")\n",
    "nb_cm.print_matrix()\n",
    "print(\"Random Forest AUC:\\n\")\n",
    "rf_cm.print_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Unsurprisingly the Naive bayes struggled, if we got rid of the highly correlated features it may do better\n",
    "- It' hard to beat 1 misclassification and the Randdom forest is just as good as the Logistic regression.\n",
    "\n",
    "It's important to note that these results are actually so good that it's normally a warning sign that you've done something wrong and you should double check the data for any data leakage (is there a feature which is actually a proxy for the target that you wouldn't really have at run time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "I'm going to use a logistic regression algorithm to create our final model.\n",
    "\n",
    "I've skipped over a lot of things that occur in a real project \n",
    "\n",
    "    - Feature Selection (Removing redundant or uninformative features)\n",
    "    - Feature Engineering (Combining or transforming raw input into features which allow the model to \n",
    "    - Model tuning (optimising the hyperparameters of the individual algorith)\n",
    "    \n",
    "All of these stages are iterative and you often find yourself going back to look at the data again based off insights gathered from looking at the misclassifications in the model and these then inform the creation of new features.\n",
    "\n",
    "The most powerful thing you can do is improve the features and get sufficient data. model tuning will get you some small gains but the most important thing is to improve your input. Always be suspicious of the results and try and fully understand what the algorithm is actually doing. I always belabour the point when helping new data scientists that just looking at accuracy isn't helpful. For a spam filter with 1% of the emails \n",
    "\n",
    "From now on we move onto focusing on reproducibility and robustness. Creating a script which is testably correct for training our final model for use in a production system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
